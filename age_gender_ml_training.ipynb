{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63539ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "import  warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abeedddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/POC_Project_LTI/uk_pmi_claims_200k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e91b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Client Name', 'Client Identifier', 'Scheme Category/ Section Name',\n",
       "       'Scheme Category/ Section Name Identifier', 'Status of Member',\n",
       "       'Claimant Unique ID', 'Claimant Year of Birth', 'Claimant Gender',\n",
       "       'Short Post Code', 'Unique Member Reference', 'Contract Start Date',\n",
       "       'Contract End Date', 'Claim ID', 'Incurred Date', 'Paid Date',\n",
       "       'Condition Code', 'Impairment Code', 'Condition Category',\n",
       "       'Treatment Type', 'Claim Type', 'Ancillary Service Type',\n",
       "       'Treatment Location', 'Provider Type', 'Admission Date',\n",
       "       'Discharge Date', 'Calculate Length of Service', 'Claim Amount',\n",
       "       'Amount Paid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574c01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Claimant Year of Birth'] = pd.to_datetime(df['Claimant Year of Birth'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034c0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Claimant Age'] = 2026 - df['Claimant Year of Birth'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1bcd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df[['Claimant Age','Claimant Gender','Claim Amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7421eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claimant Age</th>\n",
       "      <th>Claimant Gender</th>\n",
       "      <th>Claim Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59414</th>\n",
       "      <td>56</td>\n",
       "      <td>Female</td>\n",
       "      <td>322.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199248</th>\n",
       "      <td>56</td>\n",
       "      <td>Female</td>\n",
       "      <td>679.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89751</th>\n",
       "      <td>56</td>\n",
       "      <td>Male</td>\n",
       "      <td>230.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Claimant Age Claimant Gender  Claim Amount\n",
       "59414             56          Female        322.40\n",
       "199248            56          Female        679.53\n",
       "89751             56            Male        230.83"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9bca68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   Claimant Age     200000 non-null  int32  \n",
      " 1   Claimant Gender  200000 non-null  object \n",
      " 2   Claim Amount     200000 non-null  float64\n",
      "dtypes: float64(1), int32(1), object(1)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079ad53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04edc83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries availability: xgboost=True, lightgbm=False, catboost=True, shap=True, lime=True, tensorflow=True.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.base import clone\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional libraries (must be installed)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb = None\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    lgb = None\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "except Exception:\n",
    "    CatBoostRegressor = None\n",
    "try:\n",
    "    import shap\n",
    "except Exception:\n",
    "    shap = None\n",
    "try:\n",
    "    from lime.lime_tabular import LimeTabularExplainer\n",
    "except Exception:\n",
    "    LimeTabularExplainer = None\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from scikeras.wrappers import KerasRegressor\n",
    "except Exception:\n",
    "    tf = None\n",
    "    KerasRegressor = None\n",
    "\n",
    "print(\"Libraries availability: xgboost={}, lightgbm={}, catboost={}, shap={}, lime={}, tensorflow={}.\"\n",
    "      .format(xgb is not None, lgb is not None, CatBoostRegressor is not None, shap is not None, LimeTabularExplainer is not None, tf is not None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (200000, 3)\n",
      "Columns: ['Claimant Age', 'Claimant Gender', 'Claim Amount']\n",
      "Numeric cols: []\n",
      "Categorical cols: ['Claimant Gender']\n",
      "Train/test shapes: (160000, 2) (40000, 2)\n",
      "Processed feature count: 3\n",
      "\n",
      "Tuning GradientBoostingRegressor...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "GB best params: {'model__subsample': 0.8, 'model__n_estimators': 100, 'model__min_samples_split': 2, 'model__max_depth': 3, 'model__learning_rate': 0.05}\n",
      "GradientBoosting: MAE=5424.8277, RMSE=20792.8523, R2=-0.0542\n",
      "\n",
      "Tuning XGBoost...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "XGB best params: {'model__subsample': 0.8, 'model__n_estimators': 200, 'model__max_depth': 8, 'model__learning_rate': 0.1, 'model__colsample_bytree': 1.0}\n",
      "XGBoost: MAE=5424.7143, RMSE=20792.9797, R2=-0.0542\n",
      "LightGBM not available; skipping.\n",
      "\n",
      "Tuning CatBoost...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "CatBoost best params: {'model__learning_rate': 0.01, 'model__l2_leaf_reg': 7, 'model__iterations': 200, 'model__depth': 4}\n",
      "CatBoost: MAE=5424.7575, RMSE=20792.9367, R2=-0.0542\n",
      "\n",
      "Tuning Neural Network (Keras via SciKeras)...\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df = df_final\n",
    "print(\"Initial shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Set these column names according to your file:\n",
    "TARGET = \"Claim Amount\"   # target column\n",
    "# if gender and age are named differently, adjust below:\n",
    "EXPECTED_COLS = [\"Claimant Gender\", \"Claimant Age\", TARGET]\n",
    "\n",
    "for c in EXPECTED_COLS:\n",
    "    if c not in df.columns:\n",
    "        print(f\"Warning: expected column '{c}' not found in dataset. Found columns: {df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# Drop rows where target is missing\n",
    "df = df.dropna(subset=[TARGET])\n",
    "# Optionally: remove negative or impossible claims\n",
    "df = df[df[TARGET] >= 0]\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].values\n",
    "\n",
    "# Log-transform the target to reduce skew (common for claim amounts)\n",
    "y_trans = np.log1p(y)  # later we will inverse with expm1\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# If 'gender' is present but encoded as numbers, treat as categorical\n",
    "if \"gender\" in numeric_cols:\n",
    "    numeric_cols.remove(\"gender\")\n",
    "    categorical_cols.append(\"gender\")\n",
    "\n",
    "print(\"Numeric cols:\", numeric_cols)\n",
    "print(\"Categorical cols:\", categorical_cols)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "sk_version = tuple(map(int, sklearn.__version__.split(\".\")[:2]))\n",
    "\n",
    "if sk_version >= (1, 2):\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "else:\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_cols),\n",
    "    (\"cat\", categorical_transformer, categorical_cols)\n",
    "], remainder=\"drop\")  # drop any other columns\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_trans, test_size=0.2, random_state=42)\n",
    "print(\"Train/test shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "# Fit preprocessing on training\n",
    "preprocessor.fit(X_train)\n",
    "X_train_prep = preprocessor.transform(X_train)\n",
    "X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "def get_feature_names(preprocessor):\n",
    "    num_feats = numeric_cols\n",
    "    cat_feats = []\n",
    "    if categorical_cols:\n",
    "        # Get categories from the fitted OneHotEncoder\n",
    "        cat_pipeline = preprocessor.named_transformers_[\"cat\"]\n",
    "        ohe = cat_pipeline.named_steps[\"onehot\"]\n",
    "        ohe_feature_names = []\n",
    "        if hasattr(ohe, \"get_feature_names_out\"):\n",
    "            ohe_feature_names = list(ohe.get_feature_names_out(categorical_cols))\n",
    "        else:\n",
    "            # fallback\n",
    "            for i, col in enumerate(categorical_cols):\n",
    "                cats = ohe.categories_[i]\n",
    "                ohe_feature_names.extend([f\"{col}_{c}\" for c in cats])\n",
    "        cat_feats = ohe_feature_names\n",
    "    return num_feats + cat_feats\n",
    "\n",
    "feature_names = get_feature_names(preprocessor)\n",
    "print(\"Processed feature count:\", len(feature_names))\n",
    "\n",
    "def evaluate_and_print(model_name, model, X_raw, y_test_trans):\n",
    "    \"\"\"\n",
    "    model_name: str label\n",
    "    model: either a Pipeline that accepts raw X, or an estimator that expects preprocessed arrays\n",
    "    X_raw: raw dataframe (not preprocessed)  --> function will preprocess if needed\n",
    "    y_test_trans: transformed target (e.g. log1p)\n",
    "    \"\"\"\n",
    "\n",
    "    if hasattr(model, \"named_steps\"):\n",
    "        y_pred_trans = model.predict(X_raw)\n",
    "    else:\n",
    "   \n",
    "        try:\n",
    "            X_pre = preprocessor.transform(X_raw)\n",
    "        except Exception as e:\n",
    "\n",
    "            X_pre = X_raw\n",
    "        y_pred_trans = model.predict(X_pre)\n",
    "\n",
    "\n",
    "    y_pred = np.expm1(y_pred_trans)\n",
    "    y_true = np.expm1(y_test_trans)\n",
    "\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    # avoid using squared= argument to support older sklearn versions\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"{model_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, R2={r2:.4f}\")\n",
    "    return {\"model\": model_name, \"mae\": mae, \"rmse\": rmse, \"r2\": r2, \"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "RSCV_KWARGS = dict(cv=cv, n_iter=20, scoring=\"neg_mean_absolute_error\", n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "\n",
    "gb_pipeline = Pipeline(steps=[(\"pre\", preprocessor),\n",
    "                              (\"model\", GradientBoostingRegressor(random_state=42))])\n",
    "\n",
    "gb_param_dist = {\n",
    "    \"model__n_estimators\": [100, 200, 500],\n",
    "    \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"model__max_depth\": [3, 5, 8],\n",
    "    \"model__subsample\": [0.6, 0.8, 1.0],\n",
    "    \"model__min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "print(\"\\nTuning GradientBoostingRegressor...\")\n",
    "gb_search = RandomizedSearchCV(gb_pipeline, gb_param_dist, **RSCV_KWARGS)\n",
    "gb_search.fit(X_train, y_train)\n",
    "best_gb = gb_search.best_estimator_\n",
    "print(\"GB best params:\", gb_search.best_params_)\n",
    "results.append(evaluate_and_print(\"GradientBoosting\", best_gb, X_test, y_test))\n",
    "\n",
    "\n",
    "if xgb is not None:\n",
    "    xgb_pipeline = Pipeline(steps=[(\"pre\", preprocessor),\n",
    "                                   (\"model\", xgb.XGBRegressor(objective=\"reg:squarederror\", tree_method=\"auto\", random_state=42))])\n",
    "    xgb_param_dist = {\n",
    "        \"model__n_estimators\": [100, 200, 500],\n",
    "        \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"model__max_depth\": [3, 5, 8],\n",
    "        \"model__subsample\": [0.6, 0.8, 1.0],\n",
    "        \"model__colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    print(\"\\nTuning XGBoost...\")\n",
    "    xgb_search = RandomizedSearchCV(xgb_pipeline, xgb_param_dist, **RSCV_KWARGS)\n",
    "    xgb_search.fit(X_train, y_train)\n",
    "    best_xgb = xgb_search.best_estimator_\n",
    "    print(\"XGB best params:\", xgb_search.best_params_)\n",
    "    results.append(evaluate_and_print(\"XGBoost\", best_xgb, X_test, y_test))\n",
    "else:\n",
    "    print(\"XGBoost not available; skipping.\")\n",
    "\n",
    "\n",
    "if lgb is not None:\n",
    "    lgb_pipeline = Pipeline(steps=[(\"pre\", preprocessor),\n",
    "                                   (\"model\", lgb.LGBMRegressor(random_state=42))])\n",
    "    lgb_param_dist = {\n",
    "        \"model__n_estimators\": [100, 200, 500],\n",
    "        \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"model__max_depth\": [-1, 5, 8],\n",
    "        \"model__num_leaves\": [31, 63, 127],\n",
    "        \"model__subsample\": [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    print(\"\\nTuning LightGBM...\")\n",
    "    lgb_search = RandomizedSearchCV(lgb_pipeline, lgb_param_dist, **RSCV_KWARGS)\n",
    "    lgb_search.fit(X_train, y_train)\n",
    "    best_lgb = lgb_search.best_estimator_\n",
    "    print(\"LGB best params:\", lgb_search.best_params_)\n",
    "    results.append(evaluate_and_print(\"LightGBM\", best_lgb, X_test, y_test))\n",
    "else:\n",
    "    print(\"LightGBM not available; skipping.\")\n",
    "\n",
    "\n",
    "if CatBoostRegressor is not None:\n",
    " \n",
    "    cat_indices = [X_train.columns.get_loc(c) for c in categorical_cols if c in X_train.columns]\n",
    "\n",
    "    # Fit directly with Pool (CatBoost handles categorical features)\n",
    "    cat_model = CatBoostRegressor(verbose=0, random_state=42)\n",
    "    cat_param_dist = {\n",
    "        \"iterations\": [200, 500],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"depth\": [4, 6, 8],\n",
    "        \"l2_leaf_reg\": [1, 3, 7],\n",
    "    }\n",
    "\n",
    "    cat_pipeline = Pipeline(steps=[(\"pre\", preprocessor),\n",
    "                                   (\"model\", CatBoostRegressor(verbose=0, random_state=42))])\n",
    "    # Map params names\n",
    "    cat_param_dist_wrapped = {\n",
    "        \"model__iterations\": cat_param_dist[\"iterations\"],\n",
    "        \"model__learning_rate\": cat_param_dist[\"learning_rate\"],\n",
    "        \"model__depth\": cat_param_dist[\"depth\"],\n",
    "        \"model__l2_leaf_reg\": cat_param_dist[\"l2_leaf_reg\"],\n",
    "    }\n",
    "    print(\"\\nTuning CatBoost...\")\n",
    "    cat_search = RandomizedSearchCV(cat_pipeline, cat_param_dist_wrapped, **RSCV_KWARGS)\n",
    "    cat_search.fit(X_train, y_train)\n",
    "    best_cat = cat_search.best_estimator_\n",
    "    print(\"CatBoost best params:\", cat_search.best_params_)\n",
    "    results.append(evaluate_and_print(\"CatBoost\", best_cat, X_test, y_test))\n",
    "else:\n",
    "    print(\"CatBoost not available; skipping.\")\n",
    "\n",
    "\n",
    "if tf is not None:\n",
    "    try:\n",
    "        from scikeras.wrappers import KerasRegressor\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install SciKeras: pip install scikeras\")\n",
    "\n",
    "    print(\"\\nTuning Neural Network (Keras via SciKeras)...\")\n",
    "\n",
    "    def build_model(n_hidden=1, n_neurons=32, learning_rate=0.001, dropout=0.0, input_dim=None):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(n_neurons, activation=\"relu\", input_shape=(input_dim,)))\n",
    "        for _ in range(n_hidden - 1):\n",
    "            model.add(Dense(n_neurons, activation=\"relu\"))\n",
    "            if dropout > 0:\n",
    "                model.add(Dropout(dropout))\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"mae\"])\n",
    "        return model\n",
    "\n",
    "    input_dim = X_train_prep.shape[1]\n",
    "    keras_reg = KerasRegressor(\n",
    "        model=build_model,\n",
    "        model__input_dim=input_dim,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    nn_pipeline = Pipeline(steps=[(\"pre\", preprocessor),\n",
    "                                  (\"model\", keras_reg)])\n",
    "\n",
    "    nn_param_dist = {\n",
    "        \"model__model__n_hidden\": [1, 2, 3],\n",
    "        \"model__model__n_neurons\": [32, 64, 128],\n",
    "        \"model__model__learning_rate\": [1e-3, 1e-4],\n",
    "        \"model__model__dropout\": [0.0, 0.2],\n",
    "        \"model__epochs\": [50, 100],\n",
    "        \"model__batch_size\": [32, 64]\n",
    "    }\n",
    "\n",
    "    nn_search = RandomizedSearchCV(\n",
    "        nn_pipeline,\n",
    "        nn_param_dist,\n",
    "        n_iter=5,\n",
    "        cv=3,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        verbose=1,\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    nn_search.fit(X_train, y_train)\n",
    "    best_nn = nn_search.best_estimator_\n",
    "    print(\"NN best params:\", nn_search.best_params_)\n",
    "    results.append(evaluate_and_print(\"NeuralNetwork\", best_nn, X_test, y_test))\n",
    "else:\n",
    "    print(\"TensorFlow not available; skipping neural network.\")\n",
    "\n",
    "print(\"\\nSummary of results:\")\n",
    "for r in results:\n",
    "    print(r[\"model\"], f\"MAE={r['mae']:.4f}\", f\"RMSE={r['rmse']:.4f}\", f\"R2={r['r2']:.4f}\")\n",
    "\n",
    "# Save best models to disk\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "# For each trained model variable, save with joblib\n",
    "to_save = {\n",
    "    \"best_gb.pkl\": best_gb if 'best_gb' in globals() else None,\n",
    "    \"best_xgb.pkl\": best_xgb if 'best_xgb' in globals() else None,\n",
    "    \"best_lgb.pkl\": best_lgb if 'best_lgb' in globals() else None,\n",
    "    \"best_cat.pkl\": best_cat if 'best_cat' in globals() else None,\n",
    "    \"best_nn.pkl\": best_nn if 'best_nn' in globals() else None,\n",
    "}\n",
    "for fname, m in to_save.items():\n",
    "    if m is not None:\n",
    "        joblib.dump(m, os.path.join(\"models\", fname))\n",
    "        print(\"Saved\", fname)\n",
    "\n",
    "\n",
    "if shap is not None:\n",
    "\n",
    "    tree_model = None\n",
    "    tree_name = None\n",
    "    for name, var in [(\"LightGBM\", globals().get(\"best_lgb\")),\n",
    "                      (\"XGBoost\", globals().get(\"best_xgb\")),\n",
    "                      (\"CatBoost\", globals().get(\"best_cat\")),\n",
    "                      (\"GradientBoosting\", globals().get(\"best_gb\"))]:\n",
    "        if var is not None:\n",
    "            tree_model = var\n",
    "            tree_name = name\n",
    "            break\n",
    "\n",
    "    if tree_model is not None:\n",
    "        print(\"\\nRunning SHAP TreeExplainer on\", tree_name)\n",
    "        underlying_model = None\n",
    "        if hasattr(tree_model, \"named_steps\"):\n",
    "            # pipeline\n",
    "            underlying_model = tree_model.named_steps[\"model\"]\n",
    "            # Build preprocessed training matrix to explain\n",
    "            X_train_for_shap = preprocessor.transform(X_train)\n",
    "            X_test_for_shap = preprocessor.transform(X_test)\n",
    "        else:\n",
    "            underlying_model = tree_model\n",
    "            X_train_for_shap = X_train_prep\n",
    "            X_test_for_shap = X_test_prep\n",
    "\n",
    "        try:\n",
    "            explainer = shap.TreeExplainer(underlying_model)\n",
    "            shap_values = explainer.shap_values(X_test_for_shap)\n",
    "            print(\"SHAP values computed.\")\n",
    "            # Summary plot (global)\n",
    "            shap.summary_plot(shap_values, X_test_for_shap, feature_names=feature_names, show=True)\n",
    "        except Exception as e:\n",
    "            print(\"SHAP TreeExplainer failed:\", e)\n",
    "    else:\n",
    "        # fallback: KernelExplainer on one model (slower)\n",
    "        print(\"No tree model available for TreeExplainer. Attempting KernelExplainer on first available model.\")\n",
    "        any_model = None\n",
    "        if 'best_gb' in globals():\n",
    "            any_model = best_gb\n",
    "        elif 'best_xgb' in globals():\n",
    "            any_model = best_xgb\n",
    "        elif 'best_lgb' in globals():\n",
    "            any_model = best_lgb\n",
    "        elif 'best_cat' in globals():\n",
    "            any_model = best_cat\n",
    "        elif 'best_nn' in globals():\n",
    "            any_model = best_nn\n",
    "\n",
    "        if any_model is not None and shap is not None:\n",
    "    \n",
    "            def pred_func(x):\n",
    "\n",
    "                if hasattr(any_model, \"predict\"):\n",
    "                    # if any_model is a pipeline (expects raw), wrap appropriately:\n",
    "                    if hasattr(any_model, \"named_steps\"):\n",
    "                        return any_model.predict(x)\n",
    "                    else:\n",
    "                        return any_model.predict(x)\n",
    "                else:\n",
    "                    raise RuntimeError(\"Model has no predict method\")\n",
    "            try:\n",
    "                explainer = shap.KernelExplainer(pred_func, shap.sample(X_train_prep, min(100, X_train_prep.shape[0])))\n",
    "                shap_values = explainer.shap_values(X_test_prep[:50])\n",
    "                shap.summary_plot(shap_values, X_test_prep[:50], feature_names=feature_names, show=True)\n",
    "            except Exception as e:\n",
    "                print(\"SHAP KernelExplainer failed:\", e)\n",
    "else:\n",
    "    print(\"\\nSHAP not installed; skipping SHAP analysis.\")\n",
    "\n",
    "if LimeTabularExplainer is not None:\n",
    "    print(\"\\nLIME explanations for 3 test instances (if available):\")\n",
    "    # Use the processed training array and a prediction function mapping raw -> predicted continuous claim\n",
    "    explainer = LimeTabularExplainer(training_data=np.array(X_train_prep),\n",
    "                                     mode=\"regression\",\n",
    "                                     feature_names=feature_names,\n",
    "                                     verbose=False)\n",
    "\n",
    "    # Choose 3 instances from the test set to explain\n",
    "    n_local = min(3, X_test_prep.shape[0])\n",
    "    for i in range(n_local):\n",
    "        idx = i\n",
    "        exp = explainer.explain_instance(X_test_prep[idx], \n",
    "                                         lambda z: best_gb.predict(z) if 'best_gb' in globals() else best_xgb.predict(z),\n",
    "                                         num_features=min(10, len(feature_names)))\n",
    "        print(f\"--- LIME explanation for test instance {i} ---\")\n",
    "        print(exp.as_list())\n",
    "else:\n",
    "    print(\"\\nLIME not installed; skipping LIME explanations.\")\n",
    "\n",
    "\n",
    "if results:\n",
    "    best = sorted(results, key=lambda x: x[\"mae\"])[0]\n",
    "    y_true = best[\"y_true\"]\n",
    "    y_pred = best[\"y_pred\"]\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.3)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle=\"--\")\n",
    "    plt.xlabel(\"Actual claim amount\")\n",
    "    plt.ylabel(\"Predicted claim amount\")\n",
    "    plt.title(f\"{best['model']} predicted vs actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
